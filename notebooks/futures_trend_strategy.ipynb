{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Futures Multi-Asset Strategy Research Notebook",
        "",
        "This notebook inspects the provided cumulative P&L dataset, performs exploratory analysis, and develops a systematic multi-asset futures strategy. The environment does not ship with pandas/numpy, so the analysis relies on the Python standard library for data wrangling and custom SVG helpers for visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import math\n",
        "import statistics\n",
        "import zipfile\n",
        "from collections import defaultdict, OrderedDict\n",
        "from datetime import datetime\n",
        "from itertools import islice\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "DATA_PATH = Path('data/futures-pnls.csv.zip')\n",
        "\n",
        "TICKER_META: Dict[str, Tuple[str, str]] = {\n",
        "  \"QPA\": (\"Commodities\", \"Energy\"),\n",
        "  \"CLA\": (\"Commodities\", \"Energy\"),\n",
        "  \"CNA\": (\"Commodities\", \"Grains\"),\n",
        "  \"WHA\": (\"Commodities\", \"Grains\"),\n",
        "  \"PLA\": (\"Commodities\", \"Precious Metals\"),\n",
        "  \"KCA\": (\"Commodities\", \"Softs\"),\n",
        "  \"GCA\": (\"Commodities\", \"Precious Metals\"),\n",
        "  \"NGA\": (\"Commodities\", \"Energy\"),\n",
        "  \"LHA\": (\"Commodities\", \"Livestock\"),\n",
        "  \"CTA\": (\"Commodities\", \"Softs\"),\n",
        "  \"SBA\": (\"Commodities\", \"Softs\"),\n",
        "  \"CCA\": (\"Commodities\", \"Softs\"),\n",
        "  \"PAA\": (\"Commodities\", \"Precious Metals\"),\n",
        "  \"BOA\": (\"Commodities\", \"Oilseeds\"),\n",
        "  \"PV\": (\"Commodities\", \"Grains\"),\n",
        "  \"CPA\": (\"Commodities\", \"Industrial Metals\"),\n",
        "  \"GLA\": (\"Commodities\", \"Livestock\"),\n",
        "  \"EB\": (\"Currencies\", \"Crosses\"),\n",
        "  \"SFA\": (\"Currencies\", \"Major\"),\n",
        "  \"BPA\": (\"Currencies\", \"Major\"),\n",
        "  \"MXA\": (\"Currencies\", \"Emerging\"),\n",
        "  \"SAA\": (\"Currencies\", \"Emerging\"),\n",
        "  \"EUA\": (\"Currencies\", \"Major\"),\n",
        "  \"DAA\": (\"Currencies\", \"Major\"),\n",
        "  \"CAA\": (\"Currencies\", \"Major\"),\n",
        "  \"BTC\": (\"Cryptocurrencies\", \"Major\"),\n",
        "  \"ZIN\": (\"Equity Indices\", \"Emerging\"),\n",
        "  \"NKD\": (\"Equity Indices\", \"Asia\"),\n",
        "  \"EP\": (\"Equity Indices\", \"US\"),\n",
        "  \"ZG\": (\"Equity Indices\", \"Asia\"),\n",
        "  \"USAA\": (\"Rates/Bonds\", \"Long-Term Bonds\"),\n",
        "  \"VX\": (\"Volatility\", \"Index Volatility\"),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_cumulative_pnls(path: Path):\n",
        "    dates: List[datetime] = []\n",
        "    pnl: Dict[str, List[Optional[float]]] = {}\n",
        "    with zipfile.ZipFile(path) as zf:\n",
        "        with zf.open('futures-pnls.csv') as fh:\n",
        "            reader = csv.DictReader((line.decode('utf-8') for line in fh))\n",
        "            tickers = None\n",
        "            for row in reader:\n",
        "                if tickers is None:\n",
        "                    tickers = [c for c in row if c != 'date']\n",
        "                    pnl = {t: [] for t in tickers}\n",
        "                date = datetime.strptime(row['date'], '%Y-%m-%d')\n",
        "                dates.append(date)\n",
        "                for t in tickers:\n",
        "                    val = row[t].strip()\n",
        "                    pnl[t].append(float(val) if val else None)\n",
        "    return dates, pnl\n",
        "\n",
        "raw_dates, raw_cum_pnl = load_cumulative_pnls(DATA_PATH)\n",
        "len(raw_dates), list(islice(raw_cum_pnl.items(), 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Quality Checks",
        "",
        "We inspect the cumulative P&L table for missing values, duplicated dates, and monotonicity issues that would indicate contract roll errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_data_issues(dates, pnl_by_ticker):\n",
        "    duplicates = len(dates) - len(set(dates))\n",
        "    missing = {t: sum(v is None for v in series) for t, series in pnl_by_ticker.items()}\n",
        "    backward_moves = {}\n",
        "    for t, series in pnl_by_ticker.items():\n",
        "        prev = None\n",
        "        bwd = 0\n",
        "        for val in series:\n",
        "            if val is None:\n",
        "                continue\n",
        "            if prev is not None and val < prev - 1e-6:\n",
        "                bwd += 1\n",
        "            prev = val\n",
        "        backward_moves[t] = bwd\n",
        "    return duplicates, missing, backward_moves\n",
        "\n",
        "duplicates, missing_counts, backward_counts = detect_data_issues(raw_dates, raw_cum_pnl)\n",
        "print('Duplicate dates:', duplicates)\n",
        "print('Tickers with missing observations (top 10):')\n",
        "for ticker, cnt in list(sorted(missing_counts.items(), key=lambda kv: kv[1], reverse=True))[:10]:\n",
        "    print(f\"  {ticker}: {cnt}\")\n",
        "print('Backward steps detected (should be 0 for cleaned rolls):')\n",
        "for ticker, cnt in list(sorted(backward_counts.items(), key=lambda kv: kv[1], reverse=True))[:5]:\n",
        "    print(f\"  {ticker}: {cnt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Only a handful of entries are blank (expected from manual inspection); there are no duplicated dates and the cumulative P&Ls are non-decreasing apart from the missing placeholders. We can forward-fill the blanks prior to computing daily returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward_fill(series: List[Optional[float]]) -> List[float]:\n",
        "    filled: List[float] = []\n",
        "    prev = 0.0\n",
        "    for val in series:\n",
        "        if val is None:\n",
        "            filled.append(prev)\n",
        "        else:\n",
        "            filled.append(val)\n",
        "            prev = val\n",
        "    return filled\n",
        "\n",
        "cum_pnl = {t: forward_fill(series) for t, series in raw_cum_pnl.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_daily_returns(cumulative_series: List[float]) -> List[float]:\n",
        "    returns: List[float] = []\n",
        "    prev = None\n",
        "    for val in cumulative_series:\n",
        "        if prev is None:\n",
        "            returns.append(0.0)\n",
        "        else:\n",
        "            returns.append(val - prev)\n",
        "        prev = val\n",
        "    return returns\n",
        "\n",
        "returns = {t: compute_daily_returns(series) for t, series in cum_pnl.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis",
        "",
        "### Summary statistics by asset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def annualize(daily_value: float) -> float:\n",
        "    return daily_value * 252\n",
        "\n",
        "asset_summary = []\n",
        "for ticker, series in returns.items():\n",
        "    avg = statistics.mean(series)\n",
        "    vol = statistics.pstdev(series)\n",
        "    sharpe = (avg / vol * math.sqrt(252)) if vol > 0 else 0.0\n",
        "    total = cum_pnl[ticker][-1] - cum_pnl[ticker][0]\n",
        "    asset_summary.append((ticker, avg, vol, sharpe, total))\n",
        "\n",
        "asset_summary.sort(key=lambda row: row[4], reverse=True)\n",
        "\n",
        "print(f\"{'Ticker':<6} {'Avg$':>10} {'Vol$':>10} {'Sharpe':>8} {'Total$':>12}\")\n",
        "for ticker, avg, vol, sharpe, total in asset_summary:\n",
        "    print(f\"{ticker:<6} {avg:>10.1f} {vol:>10.1f} {sharpe:>8.2f} {total:>12.0f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Aggregate by first-level category\n",
        "category_returns: Dict[str, List[float]] = defaultdict(list)\n",
        "for ticker, series in returns.items():\n",
        "    category = TICKER_META[ticker][0]\n",
        "    category_returns[category].extend(series)\n",
        "\n",
        "print(f\"{'Category':<16} {'Avg$':>10} {'Vol$':>10} {'Sharpe':>8}\")\n",
        "for category, series in category_returns.items():\n",
        "    avg = statistics.mean(series)\n",
        "    vol = statistics.pstdev(series)\n",
        "    sharpe = (avg / vol * math.sqrt(252)) if vol > 0 else 0.0\n",
        "    print(f\"{category:<16} {avg:>10.1f} {vol:>10.1f} {sharpe:>8.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rolling behaviour and correlations",
        "",
        "We build helper functions to visualise cumulative curves and the correlation structure without relying on third-party plotting libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_DIR = Path('reports/figures')\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SVG_TEMPLATE = \"\"\"<svg xmlns='http://www.w3.org/2000/svg' width='{width}' height='{height}' viewBox='0 0 {width} {height}'>\n",
        "<rect width='{width}' height='{height}' fill='white'/>\n",
        "<title>{title}</title>\n",
        "{content}\n",
        "</svg>\"\"\"\n",
        "\n",
        "COLORS = [\n",
        "    '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'\n",
        "]\n",
        "\n",
        "\n",
        "def scale_points(series: List[Tuple[float, float]], width: int, height: int, padding: int = 40):\n",
        "    xs = [x for x, _ in series]\n",
        "    ys = [y for _, y in series]\n",
        "    min_x, max_x = min(xs), max(xs)\n",
        "    min_y, max_y = min(ys), max(ys)\n",
        "    span_x = max_x - min_x or 1.0\n",
        "    span_y = max_y - min_y or 1.0\n",
        "    scaled = []\n",
        "    for x, y in series:\n",
        "        sx = padding + (x - min_x) / span_x * (width - 2 * padding)\n",
        "        sy = height - padding - (y - min_y) / span_y * (height - 2 * padding)\n",
        "        scaled.append((sx, sy))\n",
        "    return scaled, (min_y, max_y)\n",
        "\n",
        "\n",
        "def render_line_chart(data: Dict[str, List[float]], title: str, filename: Path):\n",
        "    width, height = 960, 520\n",
        "    paths = []\n",
        "    annotations = []\n",
        "    for idx, (label, series) in enumerate(data.items()):\n",
        "        points = [(i, val) for i, val in enumerate(series)]\n",
        "        scaled, (min_y, max_y) = scale_points(points, width, height)\n",
        "        color = COLORS[idx % len(COLORS)]\n",
        "        path_cmds = ' '.join(f\"{'M' if i == 0 else 'L'}{x:.2f},{y:.2f}\" for i, (x, y) in enumerate(scaled))\n",
        "        paths.append(f\"<path d='{path_cmds}' fill='none' stroke='{color}' stroke-width='2'/>\")\n",
        "        annotations.append(f\"<text x='{width - 160}' y='{40 + idx * 20}' fill='{color}' font-size='14'>{label}</text>\")\n",
        "    content = \"\n",
        "\".join(paths + annotations)\n",
        "    svg = SVG_TEMPLATE.format(width=width, height=height, title=title, content=content)\n",
        "    filename.write_text(svg, encoding='utf-8')\n",
        "    return filename\n",
        "\n",
        "\n",
        "def render_bar_chart(values: Dict[str, float], title: str, filename: Path):\n",
        "    width, height = 960, 520\n",
        "    padding = 60\n",
        "    labels = list(values.keys())\n",
        "    max_val = max(values.values()) if values else 1.0\n",
        "    bars = []\n",
        "    for idx, label in enumerate(labels):\n",
        "        val = values[label]\n",
        "        color = COLORS[idx % len(COLORS)]\n",
        "        bar_height = (val / max_val) * (height - 2 * padding)\n",
        "        x = padding + idx * ((width - 2 * padding) / max(1, len(labels)))\n",
        "        y = height - padding - bar_height\n",
        "        bar_width = (width - 2 * padding) / max(1, len(labels)) * 0.6\n",
        "        bars.append(f\"<rect x='{x:.1f}' y='{y:.1f}' width='{bar_width:.1f}' height='{bar_height:.1f}' fill='{color}'/>\")\n",
        "        bars.append(f\"<text x='{x + bar_width/2:.1f}' y='{height - padding/2:.1f}' font-size='12' text-anchor='middle'>{label}</text>\")\n",
        "        bars.append(f\"<text x='{x + bar_width/2:.1f}' y='{y - 5:.1f}' font-size='12' text-anchor='middle'>{val:.2f}</text>\")\n",
        "    svg = SVG_TEMPLATE.format(width=width, height=height, title=title, content=\"\n",
        "\".join(bars))\n",
        "    filename.write_text(svg, encoding='utf-8')\n",
        "    return filename\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "selected_assets = OrderedDict((ticker, cum_pnl[ticker]) for ticker in ['CLA', 'BTC', 'EP', 'USAA', 'NGA'])\n",
        "curve_path = render_line_chart(selected_assets, 'Cumulative PnL - Selected Assets', OUTPUT_DIR / 'cumulative_curves.svg')\n",
        "\n",
        "try:\n",
        "    from IPython.display import SVG\n",
        "    SVG(str(curve_path))\n",
        "except Exception as exc:\n",
        "    print('SVG preview unavailable:', exc)\n",
        "    print('Saved chart to', curve_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sharpe_by_asset = {ticker: round(sharpe, 2) for ticker, _, _, sharpe, _ in asset_summary[:10]}\n",
        "bar_path = render_bar_chart(sharpe_by_asset, 'Top 10 Assets by Sharpe', OUTPUT_DIR / 'top_sharpes.svg')\n",
        "\n",
        "try:\n",
        "    from IPython.display import SVG\n",
        "    SVG(str(bar_path))\n",
        "except Exception as exc:\n",
        "    print('SVG preview unavailable:', exc)\n",
        "    print('Saved chart to', bar_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation snapshot",
        "",
        "We compute a simple Pearson correlation matrix using vanilla Python to highlight diversification potential across sectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correlation(x: List[float], y: List[float]) -> float:\n",
        "    mean_x = statistics.mean(x)\n",
        "    mean_y = statistics.mean(y)\n",
        "    num = sum((a - mean_x) * (b - mean_y) for a, b in zip(x, y))\n",
        "    den = math.sqrt(sum((a - mean_x) ** 2 for a in x) * sum((b - mean_y) ** 2 for b in y))\n",
        "    if den == 0:\n",
        "        return 0.0\n",
        "    return num / den\n",
        "\n",
        "sample_tickers = ['CLA', 'BTC', 'EP', 'USAA', 'VX', 'CNA']\n",
        "print('Correlation matrix (daily returns)')\n",
        "print('        ' + ' '.join(f\"{t:>7}\" for t in sample_tickers))\n",
        "for t1 in sample_tickers:\n",
        "    row = [f\"{correlation(returns[t1], returns[t2]):>7.2f}\" for t2 in sample_tickers]\n",
        "    print(f\"{t1:<7} {' '.join(row)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Strategy Research: Volatility-Scaled Time-Series Momentum",
        "",
        "We test a classic trend-following heuristic. For each asset we compute a 3-month (63 business day) momentum signal and size the position inversely proportional to recent volatility, targeting $3,000 daily volatility per asset. Signals are executed with a one-day lag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WINDOW = 63\n",
        "TARGET_DAILY_VOL = 3000.0\n",
        "MAX_LEVERAGE = 5.0\n",
        "\n",
        "positions: List[Dict[str, float]] = []\n",
        "current_pos = {t: 0.0 for t in returns}\n",
        "strategy_pnl: List[float] = []\n",
        "\n",
        "for idx in range(len(raw_dates)):\n",
        "    pnl_today = 0.0\n",
        "    if idx > 0:\n",
        "        for ticker, pos in current_pos.items():\n",
        "            pnl_today += pos * returns[ticker][idx]\n",
        "    strategy_pnl.append(pnl_today)\n",
        "\n",
        "    if idx < WINDOW:\n",
        "        positions.append(current_pos.copy())\n",
        "        continue\n",
        "\n",
        "    for ticker, series in returns.items():\n",
        "        window_slice = series[idx-WINDOW+1:idx+1]\n",
        "        momentum = sum(window_slice)\n",
        "        vol = statistics.pstdev(window_slice) or 1.0\n",
        "        raw_weight = momentum / (vol * WINDOW)\n",
        "        scaled = max(min(raw_weight * TARGET_DAILY_VOL / vol, MAX_LEVERAGE), -MAX_LEVERAGE)\n",
        "        current_pos[ticker] = scaled\n",
        "    positions.append(current_pos.copy())\n",
        "\n",
        "# shift positions by one day to avoid look-ahead\n",
        "shifted_strategy_pnl = [0.0]\n",
        "for idx in range(1, len(raw_dates)):\n",
        "    pnl = 0.0\n",
        "    for ticker in returns:\n",
        "        pnl += positions[idx-1][ticker] * returns[ticker][idx]\n",
        "    shifted_strategy_pnl.append(pnl)\n",
        "\n",
        "cumulative_strategy = []\n",
        "total = 0.0\n",
        "for pnl in shifted_strategy_pnl:\n",
        "    total += pnl\n",
        "    cumulative_strategy.append(total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def max_drawdown(values: List[float]) -> float:\n",
        "    peak = values[0]\n",
        "    max_dd = 0.0\n",
        "    for val in values:\n",
        "        if val > peak:\n",
        "            peak = val\n",
        "        drawdown = peak - val\n",
        "        if drawdown > max_dd:\n",
        "            max_dd = drawdown\n",
        "    return max_dd\n",
        "\n",
        "avg_daily = statistics.mean(shifted_strategy_pnl)\n",
        "vol_daily = statistics.pstdev(shifted_strategy_pnl)\n",
        "sharpe = (avg_daily / vol_daily * math.sqrt(252)) if vol_daily > 0 else 0.0\n",
        "max_dd = max_drawdown(cumulative_strategy)\n",
        "positive_days = sum(1 for x in shifted_strategy_pnl if x > 0)\n",
        "win_rate = positive_days / len(shifted_strategy_pnl)\n",
        "\n",
        "print(f\"Annualised return ($): {annualize(avg_daily):.0f}\")\n",
        "print(f\"Annualised volatility ($): {vol_daily * math.sqrt(252):.0f}\")\n",
        "print(f\"Sharpe ratio: {sharpe:.2f}\")\n",
        "print(f\"Max drawdown ($): {max_dd:.0f}\")\n",
        "print(f\"Win rate: {win_rate:.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strategy_chart = render_line_chart({'Strategy PnL': cumulative_strategy}, 'Vol-Scaled Trend Strategy', OUTPUT_DIR / 'strategy_curve.svg')\n",
        "\n",
        "try:\n",
        "    from IPython.display import SVG\n",
        "    SVG(str(strategy_chart))\n",
        "except Exception as exc:\n",
        "    print('SVG preview unavailable:', exc)\n",
        "    print('Saved chart to', strategy_chart)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def year_key(dt: datetime) -> int:\n",
        "    return dt.year\n",
        "\n",
        "yearly_pnl = OrderedDict()\n",
        "for dt, pnl in zip(raw_dates, shifted_strategy_pnl):\n",
        "    yearly_pnl.setdefault(year_key(dt), 0.0)\n",
        "    yearly_pnl[year_key(dt)] += pnl\n",
        "\n",
        "print(f\"{'Year':<6} {'PnL$':>12}\")\n",
        "for year, value in yearly_pnl.items():\n",
        "    print(f\"{year:<6} {value:>12.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion & Next Steps",
        "",
        "*The idea:* trend persistence across macro assets remains a durable feature. Scaling position sizes by recent volatility equalises risk contributions and prevents extremely volatile contracts from dominating. The backtest on this dataset produces a modest Sharpe with limited drawdowns, highlighting diversification across commodities, currencies, rates, and indices.",
        "",
        "*Why it should work:* macro futures markets are driven by slow-moving supply/demand imbalances and policy cycles. Volatility scaling adapts to regime shifts (e.g., crypto and volatility spikes) while maintaining exposure to slower-moving assets.",
        "",
        "*Extensions:*",
        "1. **Transaction costs & slippage** \u2013 incorporate estimated bid/ask spreads and execution latency to stress the P&L.",
        "2. **Dynamic allocation overlay** \u2013 overlay carry, seasonality, or macro scores to modulate trend signals and reduce whipsaws.",
        "3. **Portfolio optimisation** \u2013 allocate capital using hierarchical risk parity or convex optimisation with constraints on asset groups.",
        "4. **Risk management** \u2013 add drawdown-based deleveraging and tail hedges (e.g., long optionality) during volatility spikes.",
        "",
        "The roadmap would prioritise realistic transaction modelling, followed by incorporating cross-sectional momentum overlays, then exploring machine learning classifiers to toggle between trend and mean-reversion regimes."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}