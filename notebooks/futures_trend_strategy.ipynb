{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Low-Latency Multi-Asset Allocation Notebook\n",
        "\n",
        "This notebook prototypes an intraday portfolio construction workflow for 26 futures assets using minute-level data, a handcrafted LSTM forecaster, and an online mean-variance optimiser that produces updated portfolio weights at every timestep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective & Approach\n",
        "\n",
        "* **Objective:** deliver low-latency weight recommendations $w_i$ for each asset $i$ on a minute-by-minute basis.\n",
        "* **Forecasting model:** a lightweight Long Short-Term Memory (LSTM) encoder feeding a two-layer feedforward network trained to predict one-step-ahead returns.\n",
        "* **Allocator:** an online mean-variance optimiser that refreshes exponentially-weighted covariance estimates and solves for weights under a leverage cap.\n",
        "* **Data:** synthetic but stylised minute returns for 26 assets used to stress-test the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import statistics\n",
        "from typing import Dict, List, Sequence, Tuple\n",
        "\n",
        "random.seed(7)\n",
        "\n",
        "ASSETS: List[str] = [\n    'CLA',\n    'BTC',\n    'QPA',\n    'EB',\n    'SAA',\n    'PV',\n    'ZIN',\n    'ZG',\n    'BPA',\n    'MXA',\n    'CNA',\n    'SFA',\n    'WHA',\n    'PLA',\n    'KCA',\n    'EUA',\n    'DAA',\n    'CAA',\n    'LHA',\n    'CTA',\n    'SBA',\n    'CCA',\n    'NKD',\n    'USAA',\n    'GCA',\n    'NGA',\n]\n",
        "MINUTES: int = 720  # 12 hours of observations\n",
        "WINDOW: int = 20    # lookback window for the LSTM\n",
        "SCALE: float = 1000.0  # rescale returns for stable optimisation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---------- Linear algebra helpers (pure Python, dependency-light) ----------\n",
        "def zeros(length: int) -> List[float]:\n",
        "    return [0.0] * length\n",
        "\n",
        "def zero_matrix(rows: int, cols: int) -> List[List[float]]:\n",
        "    return [[0.0 for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "def vec_add(a: Sequence[float], b: Sequence[float]) -> List[float]:\n",
        "    return [x + y for x, y in zip(a, b)]\n",
        "\n",
        "def vec_mul(a: Sequence[float], b: Sequence[float]) -> List[float]:\n",
        "    return [x * y for x, y in zip(a, b)]\n",
        "\n",
        "def vec_scalar_mul(a: Sequence[float], scalar: float) -> List[float]:\n",
        "    return [scalar * x for x in a]\n",
        "\n",
        "def mat_vec_mul(matrix: Sequence[Sequence[float]], vec: Sequence[float]) -> List[float]:\n",
        "    return [sum(row[j] * vec[j] for j in range(len(vec))) for row in matrix]\n",
        "\n",
        "def mat_T_vec(matrix: Sequence[Sequence[float]], vec: Sequence[float]) -> List[float]:\n",
        "    cols = len(matrix[0]) if matrix else 0\n",
        "    result = [0.0] * cols\n",
        "    for r in range(len(matrix)):\n",
        "        for c in range(cols):\n",
        "            result[c] += matrix[r][c] * vec[r]\n",
        "    return result\n",
        "\n",
        "def outer_product(a: Sequence[float], b: Sequence[float]) -> List[List[float]]:\n",
        "    return [[x * y for y in b] for x in a]\n",
        "\n",
        "def add_matrices(a: Sequence[Sequence[float]], b: Sequence[Sequence[float]]) -> List[List[float]]:\n",
        "    return [[a[i][j] + b[i][j] for j in range(len(a[0]))] for i in range(len(a))]\n",
        "\n",
        "def scalar_matrix_mul(matrix: Sequence[Sequence[float]], scalar: float) -> List[List[float]]:\n",
        "    return [[scalar * matrix[i][j] for j in range(len(matrix[0]))] for i in range(len(matrix))]\n",
        "\n",
        "def identity_matrix(n: int, scale: float = 1.0) -> List[List[float]]:\n",
        "    mat = zero_matrix(n, n)\n",
        "    for i in range(n):\n",
        "        mat[i][i] = scale\n",
        "    return mat\n",
        "\n",
        "def max_drawdown(series: Sequence[float]) -> float:\n",
        "    peak = series[0] if series else 0.0\n",
        "    max_dd = 0.0\n",
        "    for value in series:\n",
        "        if value > peak:\n",
        "            peak = value\n",
        "        drawdown = peak - value\n",
        "        if drawdown > max_dd:\n",
        "            max_dd = drawdown\n",
        "    return max_dd\n",
        "\n",
        "def sigmoid(x: float) -> float:\n",
        "    if x >= 0:\n",
        "        z = math.exp(-x)\n",
        "        return 1.0 / (1.0 + z)\n",
        "    else:\n",
        "        z = math.exp(x)\n",
        "        return z / (1.0 + z)\n",
        "\n",
        "def tanh(x: float) -> float:\n",
        "    return math.tanh(x)\n",
        "\n",
        "def relu(x: float) -> float:\n",
        "    return x if x > 0 else 0.0\n",
        "\n",
        "def relu_derivative(x: float) -> float:\n",
        "    return 1.0 if x > 0 else 0.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---------- Synthetic minute-level multi-asset return generation ----------\n",
        "ASSET_SECTORS: Dict[str, str] = {\n    'CLA': 'Energy',\n    'BTC': 'Crypto',\n    'QPA': 'Energy',\n    'EB': 'FX',\n    'SAA': 'FX',\n    'PV': 'Grains',\n    'ZIN': 'Equity',\n    'ZG': 'Equity',\n    'BPA': 'FX',\n    'MXA': 'FX',\n    'CNA': 'Grains',\n    'SFA': 'FX',\n    'WHA': 'Grains',\n    'PLA': 'Metals',\n    'KCA': 'Softs',\n    'EUA': 'FX',\n    'DAA': 'FX',\n    'CAA': 'FX',\n    'LHA': 'Livestock',\n    'CTA': 'Softs',\n    'SBA': 'Softs',\n    'CCA': 'Softs',\n    'NKD': 'Equity',\n    'USAA': 'Rates',\n    'GCA': 'Metals',\n    'NGA': 'Energy',\n}\n",
        "\n",
        "SECTOR_NAMES = sorted({sector for sector in ASSET_SECTORS.values()})\n",
        "ASSET_PARAMS: Dict[str, Tuple[float, float, float]] = {}\n",
        "for asset in ASSETS:\n",
        "    beta_market = random.uniform(0.4, 1.1)\n",
        "    beta_sector = random.uniform(0.6, 1.4)\n",
        "    idio_vol = random.uniform(0.0004, 0.0012)\n",
        "    ASSET_PARAMS[asset] = (beta_market, beta_sector, idio_vol)\n",
        "\n",
        "minute_returns: List[List[float]] = []\n",
        "prices: Dict[str, float] = {asset: 100.0 for asset in ASSETS}\n",
        "\n",
        "for _ in range(MINUTES):\n",
        "    market_shock = random.gauss(0.0, 0.0005)\n",
        "    sector_shocks = {sector: random.gauss(0.0, 0.0007) for sector in SECTOR_NAMES}\n",
        "    minute_vector: List[float] = []\n",
        "    for asset in ASSETS:\n",
        "        beta_market, beta_sector, idio_vol = ASSET_PARAMS[asset]\n",
        "        sector = ASSET_SECTORS[asset]\n",
        "        residual = random.gauss(0.0, idio_vol)\n",
        "        ret = beta_market * market_shock + beta_sector * sector_shocks[sector] + residual\n",
        "        prices[asset] *= (1.0 + ret)\n",
        "        minute_vector.append(ret)\n",
        "    minute_returns.append(minute_vector)\n",
        "\n",
        "cumulative_curves: Dict[str, List[float]] = {asset: [] for asset in ASSETS}\n",
        "for asset_index, asset in enumerate(ASSETS):\n",
        "    total = 0.0\n",
        "    path: List[float] = []\n",
        "    for vec in minute_returns:\n",
        "        total += vec[asset_index]\n",
        "        path.append(total)\n",
        "    cumulative_curves[asset] = path\n",
        "print('Generated {} minute observations for {} assets.'.format(len(minute_returns), len(ASSETS)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Diagnostics\n",
        "\n",
        "We inspect the synthetic series to verify rough volatility levels and cross-sectional dispersion before fitting the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "asset_stats = []\n",
        "for idx, asset in enumerate(ASSETS):\n",
        "    series = [row[idx] for row in minute_returns]\n",
        "    avg = sum(series) / len(series)\n",
        "    vol = statistics.pstdev(series)\n",
        "    asset_stats.append((asset, avg, vol))\n",
        "asset_stats.sort(key=lambda x: x[2], reverse=True)\n",
        "print('{:<6} {:>12} {:>12}'.format('Asset', 'Mean (bp)', 'Vol (bp)'))\n",
        "for asset, avg, vol in asset_stats[:10]:\n",
        "    print('{:<6} {:>12.3f} {:>12.3f}'.format(asset, avg * 10000.0, vol * 10000.0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sequence Construction\n",
        "\n",
        "We transform the minute returns into overlapping windows for the LSTM. Each sample contains `WINDOW` minutes of history, scaled to stabilise gradient descent, with the following minute used as the prediction target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def scale_sequence(seq: Sequence[Sequence[float]]) -> List[List[float]]:\n",
        "    return [[value * SCALE for value in row] for row in seq]\n",
        "\n",
        "def scale_vector(vec: Sequence[float]) -> List[float]:\n",
        "    return [value * SCALE for value in vec]\n",
        "\n",
        "sequences: List[List[List[float]]] = []\n",
        "targets: List[List[float]] = []\n",
        "raw_targets: List[List[float]] = []\n",
        "for idx in range(WINDOW, len(minute_returns)):\n",
        "    history = minute_returns[idx - WINDOW:idx]\n",
        "    sequences.append(scale_sequence(history))\n",
        "    target = minute_returns[idx]\n",
        "    targets.append(scale_vector(target))\n",
        "    raw_targets.append(list(target))\n",
        "\n",
        "split_idx = int(0.7 * len(sequences))\n",
        "train_sequences = sequences[:split_idx]\n",
        "train_targets = targets[:split_idx]\n",
        "test_sequences = sequences[split_idx:]\n",
        "test_targets = targets[split_idx:]\n",
        "online_raw_returns = raw_targets[split_idx:]\n",
        "print('Training samples: {}, testing samples: {}'.format(len(train_sequences), len(test_sequences)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handcrafted LSTM + Deep Neural Forecaster\n",
        "\n",
        "To respect the lightweight constraint we implement the LSTM cell, activations, and feedforward layers from scratch using only the Python standard library. The network emits the next-minute return vector conditioned on the previous `WINDOW` minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class LSTMCell:\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.W = [[random.uniform(-0.05, 0.05) for _ in range(input_dim)] for _ in range(4 * hidden_dim)]\n",
        "        self.U = [[random.uniform(-0.05, 0.05) for _ in range(hidden_dim)] for _ in range(4 * hidden_dim)]\n",
        "        self.b = [0.0 for _ in range(4 * hidden_dim)]\n",
        "\n",
        "    def forward(self, inputs: Sequence[Sequence[float]], h_prev: List[float] = None, c_prev: List[float] = None):\n",
        "        if h_prev is None:\n",
        "            h_prev = zeros(self.hidden_dim)\n",
        "        if c_prev is None:\n",
        "            c_prev = zeros(self.hidden_dim)\n",
        "        caches = []\n",
        "        h_t = list(h_prev)\n",
        "        c_t = list(c_prev)\n",
        "        for x_t in inputs:\n",
        "            z = vec_add(vec_add(mat_vec_mul(self.W, x_t), mat_vec_mul(self.U, h_t)), self.b)\n",
        "            gate_i = [sigmoid(z[idx]) for idx in range(0, self.hidden_dim)]\n",
        "            gate_f = [sigmoid(z[self.hidden_dim + idx]) for idx in range(0, self.hidden_dim)]\n",
        "            gate_g = [tanh(z[2 * self.hidden_dim + idx]) for idx in range(0, self.hidden_dim)]\n",
        "            gate_o = [sigmoid(z[3 * self.hidden_dim + idx]) for idx in range(0, self.hidden_dim)]\n",
        "            c_new = [gate_f[idx] * c_t[idx] + gate_i[idx] * gate_g[idx] for idx in range(self.hidden_dim)]\n",
        "            h_new = [gate_o[idx] * tanh(c_new[idx]) for idx in range(self.hidden_dim)]\n",
        "            caches.append({\n",
        "                'x': list(x_t),\n",
        "                'h_prev': list(h_t),\n",
        "                'c_prev': list(c_t),\n",
        "                'i': gate_i,\n",
        "                'f': gate_f,\n",
        "                'g': gate_g,\n",
        "                'o': gate_o,\n",
        "                'c': list(c_new),\n",
        "                'h': list(h_new)\n",
        "            })\n",
        "            h_t = h_new\n",
        "            c_t = c_new\n",
        "        return h_t, c_t, caches\n",
        "\n",
        "    def backward(self, grad_h: Sequence[float], grad_c: Sequence[float], caches: Sequence[dict]):\n",
        "        grad_W = zero_matrix(len(self.W), len(self.W[0]))\n",
        "        grad_U = zero_matrix(len(self.U), len(self.U[0]))\n",
        "        grad_b = [0.0 for _ in range(len(self.b))]\n",
        "        dh_next = list(grad_h)\n",
        "        dc_next = list(grad_c)\n",
        "        for cache in reversed(caches):\n",
        "            tanh_c = [tanh(value) for value in cache['c']]\n",
        "            dh_total = list(dh_next)\n",
        "            do = [dh_total[idx] * tanh_c[idx] for idx in range(self.hidden_dim)]\n",
        "            do_raw = [do[idx] * cache['o'][idx] * (1.0 - cache['o'][idx]) for idx in range(self.hidden_dim)]\n",
        "            dc = [dc_next[idx] + dh_total[idx] * cache['o'][idx] * (1.0 - tanh_c[idx] ** 2) for idx in range(self.hidden_dim)]\n",
        "            df = [dc[idx] * cache['c_prev'][idx] for idx in range(self.hidden_dim)]\n",
        "            df_raw = [df[idx] * cache['f'][idx] * (1.0 - cache['f'][idx]) for idx in range(self.hidden_dim)]\n",
        "            di = [dc[idx] * cache['g'][idx] for idx in range(self.hidden_dim)]\n",
        "            di_raw = [di[idx] * cache['i'][idx] * (1.0 - cache['i'][idx]) for idx in range(self.hidden_dim)]\n",
        "            dg = [dc[idx] * cache['i'][idx] for idx in range(self.hidden_dim)]\n",
        "            dg_raw = [dg[idx] * (1.0 - cache['g'][idx] ** 2) for idx in range(self.hidden_dim)]\n",
        "            dz = di_raw + df_raw + dg_raw + do_raw\n",
        "            outer_x = outer_product(dz, cache['x'])\n",
        "            outer_h = outer_product(dz, cache['h_prev'])\n",
        "            for row in range(len(self.W)):\n",
        "                for col in range(len(self.W[0])):\n",
        "                    grad_W[row][col] += outer_x[row][col]\n",
        "            for row in range(len(self.U)):\n",
        "                for col in range(len(self.U[0])):\n",
        "                    grad_U[row][col] += outer_h[row][col]\n",
        "            for idx in range(len(self.b)):\n",
        "                grad_b[idx] += dz[idx]\n",
        "            dh_prev = mat_T_vec(self.U, dz)\n",
        "            dc_prev = [dc[idx] * cache['f'][idx] for idx in range(self.hidden_dim)]\n",
        "            dh_next = dh_prev\n",
        "            dc_next = dc_prev\n",
        "        return grad_W, grad_U, grad_b\n",
        "\n",
        "    def apply_gradients(self, grad_W: Sequence[Sequence[float]], grad_U: Sequence[Sequence[float]], grad_b: Sequence[float], lr: float):\n",
        "        for row in range(len(self.W)):\n",
        "            for col in range(len(self.W[0])):\n",
        "                self.W[row][col] -= lr * grad_W[row][col]\n",
        "        for row in range(len(self.U)):\n",
        "            for col in range(len(self.U[0])):\n",
        "                self.U[row][col] -= lr * grad_U[row][col]\n",
        "        for idx in range(len(self.b)):\n",
        "            self.b[idx] -= lr * grad_b[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class LinearLayer:\n",
        "    def __init__(self, input_dim: int, output_dim: int):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.W = [[random.uniform(-0.05, 0.05) for _ in range(input_dim)] for _ in range(output_dim)]\n",
        "        self.b = [0.0 for _ in range(output_dim)]\n",
        "        self.last_input: List[float] = []\n",
        "\n",
        "    def forward(self, vec_in: Sequence[float]) -> List[float]:\n",
        "        self.last_input = list(vec_in)\n",
        "        return vec_add(mat_vec_mul(self.W, vec_in), self.b)\n",
        "\n",
        "    def backward(self, grad_output: Sequence[float], lr: float) -> List[float]:\n",
        "        grad_input = mat_T_vec(self.W, grad_output)\n",
        "        for i in range(self.output_dim):\n",
        "            for j in range(self.input_dim):\n",
        "                self.W[i][j] -= lr * grad_output[i] * self.last_input[j]\n",
        "            self.b[i] -= lr * grad_output[i]\n",
        "        return grad_input\n",
        "\n",
        "class DeepSequenceModel:\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
        "        self.lstm = LSTMCell(input_dim, hidden_dim)\n",
        "        self.fc1 = LinearLayer(hidden_dim, hidden_dim)\n",
        "        self.fc2 = LinearLayer(hidden_dim, output_dim)\n",
        "        self.last_relu_input: List[float] = []\n",
        "\n",
        "    def forward(self, sequence: Sequence[Sequence[float]]):\n",
        "        h_last, c_last, caches = self.lstm.forward(sequence)\n",
        "        z1 = self.fc1.forward(h_last)\n",
        "        self.last_relu_input = list(z1)\n",
        "        a1 = [relu(value) for value in z1]\n",
        "        output = self.fc2.forward(a1)\n",
        "        return output, caches\n",
        "\n",
        "    def backward(self, grad_output: Sequence[float], caches: Sequence[dict], lr: float):\n",
        "        grad_a1 = self.fc2.backward(grad_output, lr)\n",
        "        grad_z1 = [grad_a1[i] * relu_derivative(self.last_relu_input[i]) for i in range(len(grad_a1))]\n",
        "        grad_h = self.fc1.backward(grad_z1, lr)\n",
        "        grad_c = zeros(len(grad_h))\n",
        "        grad_W, grad_U, grad_b = self.lstm.backward(grad_h, grad_c, caches)\n",
        "        self.lstm.apply_gradients(grad_W, grad_U, grad_b, lr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def mean_squared_error(pred: Sequence[float], target: Sequence[float]) -> float:\n",
        "    return sum((p - t) ** 2 for p, t in zip(pred, target)) / len(pred)\n",
        "\n",
        "def train_model(model: DeepSequenceModel, sequences: Sequence[Sequence[Sequence[float]]], targets: Sequence[Sequence[float]], epochs: int = 8, lr: float = 0.0005):\n",
        "    for epoch in range(epochs):\n",
        "        paired = list(zip(sequences, targets))\n",
        "        random.shuffle(paired)\n",
        "        total_loss = 0.0\n",
        "        for seq, target in paired:\n",
        "            pred, caches = model.forward(seq)\n",
        "            loss = mean_squared_error(pred, target)\n",
        "            grad = [2.0 * (pred[i] - target[i]) / len(pred) for i in range(len(pred))]\n",
        "            model.backward(grad, caches, lr)\n",
        "            total_loss += loss\n",
        "        avg_loss = total_loss / len(paired)\n",
        "        print('Epoch {:>2}: training loss = {:.6f}'.format(epoch + 1, avg_loss))\n",
        "\n",
        "model = DeepSequenceModel(input_dim=len(ASSETS), hidden_dim=16, output_dim=len(ASSETS))\n",
        "train_model(model, train_sequences, train_targets, epochs=10, lr=0.0003)\n",
        "\n",
        "def evaluate(model: DeepSequenceModel, sequences: Sequence[Sequence[Sequence[float]]], targets: Sequence[Sequence[float]]):\n",
        "    total_loss = 0.0\n",
        "    for seq, target in zip(sequences, targets):\n",
        "        pred, _ = model.forward(seq)\n",
        "        total_loss += mean_squared_error(pred, target)\n",
        "    return total_loss / len(sequences) if sequences else 0.0\n",
        "\n",
        "test_loss = evaluate(model, test_sequences, test_targets)\n",
        "print('Test MSE: {:.6f}'.format(test_loss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Online Mean-Variance Allocation\n",
        "\n",
        "At run-time we pair the model forecasts with an exponentially-weighted covariance estimator. We solve the unconstrained mean-variance problem $\\max_w \\mu^\\top w - \\lambda w^\\top \\Sigma w$ with a leverage cap to maintain implementability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def gauss_jordan_inverse(matrix: Sequence[Sequence[float]]) -> List[List[float]]:\n",
        "    n = len(matrix)\n",
        "    aug = [list(row) + [1.0 if i == j else 0.0 for j in range(n)] for i, row in enumerate(matrix)]\n",
        "    for col in range(n):\n",
        "        pivot_row = max(range(col, n), key=lambda r: abs(aug[r][col]))\n",
        "        if abs(aug[pivot_row][col]) < 1e-9:\n",
        "            raise ValueError('Matrix near-singular during inversion')\n",
        "        if pivot_row != col:\n",
        "            aug[col], aug[pivot_row] = aug[pivot_row], aug[col]\n",
        "        pivot = aug[col][col]\n",
        "        for j in range(2 * n):\n",
        "            aug[col][j] /= pivot\n",
        "        for row in range(n):\n",
        "            if row == col:\n",
        "                continue\n",
        "            factor = aug[row][col]\n",
        "            if factor == 0.0:\n",
        "                continue\n",
        "            for j in range(2 * n):\n",
        "                aug[row][j] -= factor * aug[col][j]\n",
        "    return [row[n:] for row in aug]\n",
        "\n",
        "def update_covariance(current: Sequence[Sequence[float]], obs: Sequence[float], decay: float) -> List[List[float]]:\n",
        "    observation_outer = outer_product(obs, obs)\n",
        "    decayed = scalar_matrix_mul(current, decay)\n",
        "    increment = scalar_matrix_mul(observation_outer, 1.0 - decay)\n",
        "    return add_matrices(decayed, increment)\n",
        "\n",
        "def solve_weights(mu: Sequence[float], covariance: Sequence[Sequence[float]], risk_aversion: float, leverage_cap: float) -> List[float]:\n",
        "    dim = len(mu)\n",
        "    regularised = [list(row) for row in covariance]\n",
        "    for i in range(dim):\n",
        "        regularised[i][i] += 1e-6\n",
        "    inv_cov = gauss_jordan_inverse(regularised)\n",
        "    raw = mat_vec_mul(inv_cov, mu)\n",
        "    weights = [value / max(risk_aversion, 1e-6) for value in raw]\n",
        "    leverage = sum(abs(w) for w in weights)\n",
        "    if leverage > leverage_cap and leverage > 0:\n",
        "        scale = leverage_cap / leverage\n",
        "        weights = [w * scale for w in weights]\n",
        "    return weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "decay = 0.94\n",
        "risk_aversion = 8.0\n",
        "leverage_cap = 3.0\n",
        "covariance = identity_matrix(len(ASSETS), scale=1e-6)\n",
        "prev_weights = [0.0 for _ in ASSETS]\n",
        "realised_pnls: List[float] = []\n",
        "cumulative_curve: List[float] = []\n",
        "weights_history: List[List[float]] = []\n",
        "turnover = 0.0\n",
        "cumulative = 0.0\n",
        "for seq, target_scaled, realised in zip(test_sequences, test_targets, online_raw_returns):\n",
        "    forecast_scaled, _ = model.forward(seq)\n",
        "    forecast = [value / SCALE for value in forecast_scaled]\n",
        "    covariance = update_covariance(covariance, realised, decay)\n",
        "    weights = solve_weights(forecast, covariance, risk_aversion, leverage_cap)\n",
        "    pnl = sum(weights[i] * realised[i] for i in range(len(ASSETS)))\n",
        "    realised_pnls.append(pnl)\n",
        "    cumulative += pnl\n",
        "    cumulative_curve.append(cumulative)\n",
        "    weights_history.append(weights)\n",
        "    turnover += sum(abs(weights[i] - prev_weights[i]) for i in range(len(ASSETS))) / 2.0\n",
        "    prev_weights = weights\n",
        "\n",
        "minutes_in_year = 252 * 390\n",
        "avg_pnl = sum(realised_pnls) / len(realised_pnls)\n",
        "vol_pnl = statistics.pstdev(realised_pnls) if len(realised_pnls) > 1 else 0.0\n",
        "annual_return = avg_pnl * minutes_in_year\n",
        "annual_vol = vol_pnl * math.sqrt(minutes_in_year)\n",
        "sharpe = (annual_return / annual_vol) if annual_vol > 0 else 0.0\n",
        "dd = max_drawdown(cumulative_curve) if cumulative_curve else 0.0\n",
        "avg_turnover = turnover / max(len(weights_history), 1)\n",
        "print('Annualised return: {:.2f} bp'.format(annual_return * 100.0))\n",
        "print('Annualised vol   : {:.2f} bp'.format(annual_vol * 100.0))\n",
        "print('Sharpe ratio     : {:.2f}'.format(sharpe))\n",
        "print('Max drawdown     : {:.2f} bp'.format(dd * 100.0))\n",
        "print('Average turnover : {:.4f}'.format(avg_turnover))\n",
        "\n",
        "print('Latest weight snapshot:')\n",
        "if weights_history:\n",
        "    latest = weights_history[-1]\n",
        "    for asset, weight in list(zip(ASSETS, latest))[:10]:\n",
        "        print('{}: {:+.4f}'.format(asset, weight))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion & Extension Roadmap\n",
        "\n",
        "**Lifecycle.** The prototype demonstrates that a minimal pure-Python stack can ingest minute updates, propagate them through an LSTM forecaster, and deliver refreshed weights via an online mean-variance solver. Training remains lightweight (seconds) because the architecture is compact and the optimiser avoids matrix factorisations beyond a Gauss-Jordan inverse of a 26\u00d726 matrix.\n",
        "\n",
        "**Potential enhancements.**\n",
        "1. Replace the handcrafted autodiff with a production-grade library (e.g., PyTorch) to unlock richer architectures, ensembling, and GPU acceleration.\n",
        "2. Incorporate microstructure features\u2014order-book imbalance, realised volatility, volume\u2014to improve the predictive signal.\n",
        "3. Impose execution-aware constraints (transaction costs, market impact) and extend the allocator to handle cash targets, turnover budgets, and risk parity objectives.\n",
        "4. Deploy the estimator in a streaming environment (Kafka / Redis queue) with asynchronous model refresh to meet true low-latency requirements.\n"
      ]
    }
  ]
}