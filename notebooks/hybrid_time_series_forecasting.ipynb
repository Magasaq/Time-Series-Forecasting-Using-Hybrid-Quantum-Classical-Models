{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hybrid Quantum-Classical Time Series Forecasting\n",
        "\n",
        "This notebook builds a hybrid quantum-classical model for time series forecasting on a synthetic dataset and compares it with a classical baseline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "Install required libraries and configure the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "!pip -q install pennylane torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu > /dev/null\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pennylane as qml\n",
        "import torch\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n",
        "Generate a synthetic time series and prepare training/test windows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Generate a synthetic time series that combines trend, seasonal, and noise components\n",
        "n_points = 1800\n",
        "time = np.arange(n_points)\n",
        "series = 0.05 * time + np.sin(0.2 * time) + 0.5 * np.sin(0.05 * time) + 0.3 * np.cos(0.15 * time)\n",
        "noise = np.random.normal(scale=0.2, size=n_points)\n",
        "series += noise\n",
        "\n",
        "series_df = pd.DataFrame({\"t\": time, \"value\": series})\n",
        "series_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(series_df[\"t\"], series_df[\"value\"], label=\"Synthetic series\")\n",
        "ax.set_xlabel(\"Time step\")\n",
        "ax.set_ylabel(\"Value\")\n",
        "ax.set_title(\"Synthetic time series used for forecasting\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Scaling\n",
        "scaler = MinMaxScaler()\n",
        "series_scaled = scaler.fit_transform(series_df[[\"value\"]]).flatten()\n",
        "\n",
        "# Windowing helper\n",
        "def create_windows(data, window_size, horizon=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - window_size - horizon + 1):\n",
        "        X.append(data[i : i + window_size])\n",
        "        y.append(data[i + window_size : i + window_size + horizon])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "window_size = 32\n",
        "horizon = 1\n",
        "X, y = create_windows(series_scaled, window_size, horizon)\n",
        "\n",
        "split_idx = int(0.75 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).unsqueeze(-1)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size)\n",
        "\n",
        "X_train_tensor.shape, y_train_tensor.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Classical Baseline (LSTM)\n",
        "Train a purely classical LSTM baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class LSTMForecaster(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=32, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, horizon)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        last_hidden = lstm_out[:, -1, :]\n",
        "        return self.fc(last_hidden)\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=30, lr=1e-3):\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "    history = {\"train_loss\": [], \"val_loss\": []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                preds = model(xb)\n",
        "                loss = criterion(preds, yb)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        history[\"train_loss\"].append(np.mean(train_losses))\n",
        "        history[\"val_loss\"].append(np.mean(val_losses))\n",
        "        if epoch % 5 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:02d} | Train Loss: {history['train_loss'][-1]:.4f} | Val Loss: {history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "baseline_model = LSTMForecaster(hidden_size=32)\n",
        "baseline_history = train_model(baseline_model, train_loader, val_loader, epochs=35, lr=5e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate(model, X_tensor, y_true_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        preds = model(X_tensor).cpu().numpy().flatten()\n",
        "    y_true = y_true_tensor.cpu().numpy().flatten()\n",
        "    mse = mean_squared_error(y_true, preds)\n",
        "    rmse = math.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, preds)\n",
        "    return preds, {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae}\n",
        "\n",
        "baseline_preds, baseline_metrics = evaluate(baseline_model, X_test_tensor, y_test_tensor)\n",
        "baseline_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Plot baseline predictions vs actuals on the test set\n",
        "actual = scaler.inverse_transform(y_test)\n",
        "predicted = scaler.inverse_transform(baseline_preds.reshape(-1, 1))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(actual, label=\"Actual\", linewidth=2)\n",
        "ax.plot(predicted, label=\"LSTM Baseline\", linestyle='--')\n",
        "ax.set_title(\"Baseline LSTM Forecast vs Actual\")\n",
        "ax.set_xlabel(\"Test time step\")\n",
        "ax.set_ylabel(\"Value\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Hybrid Quantum-Classical Model\n",
        "Build an LSTM combined with a variational quantum circuit layer implemented with PennyLane.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class QuantumLSTMForecaster(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=32, n_qubits=4, q_depth=2):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_qubits = n_qubits\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
        "        self.to_qubits = nn.Linear(hidden_size, n_qubits)\n",
        "\n",
        "        dev = qml.device(\"default.qubit.torch\", wires=n_qubits)\n",
        "\n",
        "        @qml.qnode(dev, interface=\"torch\")\n",
        "        def circuit(inputs, weights):\n",
        "            qml.templates.AngleEmbedding(inputs, wires=range(n_qubits), rotation='Y')\n",
        "            qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "            return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
        "\n",
        "        weight_shapes = {\"weights\": (q_depth, n_qubits, 3)}\n",
        "        self.quantum_layer = qml.qnn.TorchLayer(circuit, weight_shapes)\n",
        "        self.readout = nn.Sequential(\n",
        "            nn.Linear(n_qubits, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, horizon)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        last_hidden = lstm_out[:, -1, :]\n",
        "        qubit_inputs = torch.tanh(self.to_qubits(last_hidden))\n",
        "        quantum_out = self.quantum_layer(qubit_inputs)\n",
        "        return self.readout(quantum_out)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "quantum_model = QuantumLSTMForecaster(hidden_size=32, n_qubits=4, q_depth=2)\n",
        "quantum_history = train_model(quantum_model, train_loader, val_loader, epochs=35, lr=5e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "quantum_preds, quantum_metrics = evaluate(quantum_model, X_test_tensor, y_test_tensor)\n",
        "quantum_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "actual = scaler.inverse_transform(y_test)\n",
        "quantum_predicted = scaler.inverse_transform(quantum_preds.reshape(-1, 1))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.plot(actual, label=\"Actual\", linewidth=2)\n",
        "ax.plot(quantum_predicted, label=\"Hybrid QNN\", linestyle='--')\n",
        "ax.set_title(\"Hybrid Quantum-Classical Forecast vs Actual\")\n",
        "ax.set_xlabel(\"Test time step\")\n",
        "ax.set_ylabel(\"Value\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Circuit Depth Experiments\n",
        "Evaluate different quantum circuit depths and compare metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "results = []\n",
        "for depth in [1, 2, 3]:\n",
        "    print(f\"\\nTraining hybrid model with circuit depth = {depth}\")\n",
        "    model = QuantumLSTMForecaster(hidden_size=32, n_qubits=4, q_depth=depth)\n",
        "    train_model(model, train_loader, val_loader, epochs=25, lr=5e-3)\n",
        "    _, metrics = evaluate(model, X_test_tensor, y_test_tensor)\n",
        "    results.append({\"Circuit depth\": depth, **metrics})\n",
        "\n",
        "depth_results_df = pd.DataFrame(results)\n",
        "depth_results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comparison Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "comparison_df = pd.DataFrame([\n",
        "    {\"Model\": \"LSTM Baseline\", **baseline_metrics},\n",
        "    {\"Model\": \"Hybrid QNN (depth=2)\", **quantum_metrics}\n",
        "])\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "metrics_to_plot = [\"RMSE\", \"MAE\"]\n",
        "width = 0.35\n",
        "x = np.arange(len(metrics_to_plot))\n",
        "\n",
        "baseline_vals = [baseline_metrics[m] for m in metrics_to_plot]\n",
        "quantum_vals = [quantum_metrics[m] for m in metrics_to_plot]\n",
        "\n",
        "ax.bar(x - width/2, baseline_vals, width, label=\"LSTM Baseline\")\n",
        "ax.bar(x + width/2, quantum_vals, width, label=\"Hybrid QNN\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics_to_plot)\n",
        "ax.set_ylabel(\"Error\")\n",
        "ax.set_title(\"Error comparison\")\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The notebook above can be extended further by adjusting hyperparameters, experimenting with different quantum circuits, or changing the dataset.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}