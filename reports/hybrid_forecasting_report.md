# Hybrid Quantum-Classical Time Series Forecasting Report

## Overview
This report summarizes the development of a hybrid quantum-classical model for time series forecasting. The accompanying notebook (`notebooks/hybrid_time_series_forecasting.ipynb`) walks through data preparation, model construction, training workflows, and evaluation utilities that compare a classical LSTM baseline with a quantum-enhanced LSTM.

## Dataset and Preprocessing
- **Dataset**: A synthetic univariate series composed of linear trend, multiple sinusoidal components, and additive Gaussian noise (1,800 time steps).
- **Scaling**: Min-max normalization to the range `[0, 1]` for stable optimization.
- **Windowing**: Sliding windows of length 32 with a forecast horizon of one step to create supervised learning samples.
- **Splits**: Chronological split with 75% for training and 25% for testing. PyTorch `DataLoader` objects handle batching and shuffling for the training subset.

## Models
### Classical Baseline
- Single-layer LSTM with 32 hidden units and a feed-forward projection head.
- Optimized via Adam (`lr=5e-3`) using mean squared error loss.

### Hybrid Quantum-Classical Model
- Shares the LSTM encoder backbone with the baseline.
- Projects the final hidden state to four qubit rotation angles via a linear layer with `tanh` activation.
- Employs a PennyLane variational circuit using `StronglyEntanglingLayers` across 4 qubits with configurable depth.
- Quantum expectation values are mapped back to forecast space through a classical readout network.

## Training and Evaluation
- Utility functions in the notebook train models for a configurable number of epochs and capture loss curves.
- Evaluation computes MSE, RMSE, and MAE on the held-out test set along with visualization of predictions versus ground truth.
- Additional experiments sweep circuit depths (1â€“3 layers) to study trade-offs between model expressivity and generalization.

## Key Insights and Observations
- The hybrid approach is designed to investigate whether quantum feature transformations can reduce forecast error compared with the classical baseline.
- Expectation: moderate circuit depths (e.g., depth=2) should balance performance and training stability; excessive depth may overfit or slow convergence.
- Visualizations illustrate qualitative differences between forecast trajectories, enabling inspection of phase alignment and amplitude capture.

## Challenges and Improvements
- **Environment limitations**: The execution environment used to generate this report lacks direct internet access, preventing automatic installation of dependencies such as PyTorch and PennyLane. The notebook includes installation commands that should be executed in an environment with package access before running the experiments.
- **Possible enhancements**:
  - Explore alternative quantum encodings (e.g., amplitude encoding or data re-uploading techniques).
  - Integrate classical regularization strategies (dropout, weight decay) to stabilize training.
  - Evaluate on real-world datasets (e.g., energy demand, stock prices) to assess robustness.
  - Extend comparisons with additional classical baselines (ARIMA, Transformer) for broader benchmarking.

## Contribution of the Quantum Circuit
Running the notebook in a fully provisioned environment allows direct comparison of forecast metrics to determine whether the quantum circuit yields measurable improvements. The provided framework streamlines experimentation so researchers can quantify the contribution of the variational circuit relative to the classical LSTM baseline.
